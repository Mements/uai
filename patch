diff --git a/uai.ts b/uai.ts
index 1234567..abcdefg 100644
--- a/uai.ts
+++ b/uai.ts
@@ -33,10 +33,17 @@ export interface AgentConfig<I extends z.ZodObject<any>, O extends z.ZodObject<
 
 // ### Progress Callback Types
 export interface ProgressUpdate {
-  stage: "server_selection" | "tool_discovery" | "tool_invocation" | "response_generation";
+  stage: "server_selection" | "tool_discovery" | "tool_invocation" | "response_generation" | "streaming";
   message: string;
   data?: any;
 }
 
+export interface StreamingUpdate {
+  stage: "streaming";
+  field: string;
+  value: string;
+}
+
 export type ProgressCallback = (update: ProgressUpdate) => void;
+export type StreamingCallback = (update: StreamingUpdate) => void;
 
 // ### XML Utilities
@@ -102,10 +109,11 @@ function xmlToObj(xmlContent: string): any {
 /** Calls an LLM API with measurement for logging. */
 async function callLLM(
   llm: LLMType,
   messages: Array<{ role: string; content: string }>,
   options: { temperature?: number; maxTokens?: number } = {},
-  measureFn?: typeof measure
+  measureFn?: typeof measure,
+  streamingCallback?: StreamingCallback
 ): Promise<string> {
   const executeCall = async (measure: typeof measure) => {
     const { temperature = 0.7, maxTokens = 4000 } = options;
@@ -119,17 +127,20 @@ async function callLLM(
       headers["x-api-key"] = process.env.ANTHROPIC_API_KEY!;
       headers["anthropic-version"] = "2023-06-01";
       url = "https://api.anthropic.com/v1/messages";
-      body = { model: llm, max_tokens: maxTokens, messages };
+      body = { model: llm, max_tokens: maxTokens, messages, stream: !!streamingCallback };
     } else if (llm.includes("deepseek")) {
       headers["Authorization"] = `Bearer ${process.env.DEEPSEEK_API_KEY}`;
       url = "https://api.deepseek.com/v1/chat/completions";
-      body = { model: llm, temperature, messages, max_tokens: maxTokens };
+      body = { model: llm, temperature, messages, max_tokens: maxTokens, stream: !!streamingCallback };
     } else {
       headers["Authorization"] = `Bearer ${process.env.OPENAI_API_KEY}`;
       url = "https://api.openai.com/v1/chat/completions";
-      body = { model: llm, temperature, messages, max_tokens: maxTokens };
+      body = { model: llm, temperature, messages, max_tokens: maxTokens, stream: !!streamingCallback };
     }
+
     const requestBodyStr = JSON.stringify(body);
+    
+    if (!streamingCallback) {
     const response = await measure(
       async () => {
         const res = await fetch(url, {
@@ -144,7 +155,98 @@ async function callLLM(
       `HTTP ${llm} API call - Body: ${requestBodyStr.substring(0, 200)}...`
     );
     const data = await response.json();
     return llm.includes("claude") ? data.content[0].text : data.choices[0].message.content;
+    } else {
+      // Streaming response handling
+      const response = await measure(
+        async () => {
+          const res = await fetch(url, {
+            method: "POST",
+            headers,
+            body: requestBodyStr,
+          });
+          if (!res.ok) {
+            const errorText = await res.text();
+            throw new Error(`LLM API error: ${errorText}`);
+          }
+          return res;
+        },
+        `HTTP ${llm} streaming API call - Body: ${requestBodyStr.substring(0, 200)}...`
+      );
+
+      const reader = response.body?.getReader();
+      if (!reader) {
+        throw new Error("No readable stream available");
+      }
+
+      const decoder = new TextDecoder();
+      let fullResponse = "";
+      let currentField = "";
+      let currentValue = "";
+      let insideTag = false;
+      let buffer = "";
+
+      try {
+        while (true) {
+          const { done, value } = await reader.read();
+          if (done) break;
+
+          const chunk = decoder.decode(value, { stream: true });
+          buffer += chunk;
+
+          // Process complete lines for different providers
+          const lines = buffer.split('\n');
+          buffer = lines.pop() || ''; // Keep incomplete line in buffer
+
+          for (const line of lines) {
+            let content = '';
+            
+            if (llm.includes("claude")) {
+              // Anthropic streaming format
+              if (line.startsWith('data: ') && !line.includes('[DONE]')) {
+                try {
+                  const data = JSON.parse(line.slice(6));
+                  if (data.type === 'content_block_delta' && data.delta?.text) {
+                    content = data.delta.text;
+                  }
+                } catch (e) {
+                  continue;
+                }
+              }
+            } else {
+              // OpenAI/DeepSeek streaming format
+              if (line.startsWith('data: ') && !line.includes('[DONE]')) {
+                try {
+                  const data = JSON.parse(line.slice(6));
+                  if (data.choices?.[0]?.delta?.content) {
+                    content = data.choices[0].delta.content;
+                  }
+                } catch (e) {
+                  continue;
+                }
+              }
+            }
+
+            if (content) {
+              fullResponse += content;
+              
+              // Parse XML tags to detect field changes
+              for (const char of content) {
+                if (char === '<') {
+                  insideTag = true;
+                  currentValue = "";
+                } else if (char === '>' && insideTag) {
+                  insideTag = false;
+                  if (!currentValue.startsWith('/')) {
+                    currentField = currentValue;
+                    currentValue = "";
+                  }
+                } else if (insideTag) {
+                  currentValue += char;
+                } else if (currentField && char !== '\n') {
+                  currentValue += char;
+                  streamingCallback({ stage: "streaming", field: currentField, value: currentValue });
+                }
+              }
+            }
+          }
+        }
+      } finally {
+        reader.releaseLock();
+      }
+
+      return fullResponse;
+    }
   };
   return measureFn
     ? await measureFn(executeCall, `LLM call to ${llm}`)
@@ -254,7 +356,8 @@ export class Agent<I extends z.ZodObject<any>, O extends z.ZodObject<any>> {
           stage: "response_generation",
           message: "Generating final response...",
         });
+        
         try {
           const response = await measure(
-            async (measure) => await this.generateResponse(validatedInput, toolResults, measure),
+            async (measure) => await this.generateResponse(validatedInput, toolResults, measure, progressCallback),
             "Generate AI response with toolResults: " + JSON.stringify(toolResults),
           );
           return await measure(
@@ -409,7 +512,16 @@ export class Agent<I extends z.ZodObject<any>, O extends z.ZodObject<any>> {
   }
 
   /** Generates the final response based on input and tool results. */
-  private async generateResponse(input: any, toolResults: Record<string, any>, measureFn: typeof measure): Promise<any> {
+  private async generateResponse(
+    input: any, 
+    toolResults: Record<string, any>, 
+    measureFn: typeof measure, 
+    progressCallback?: ProgressCallback
+  ): Promise<any> {
+    const streamingCallback: StreamingCallback | undefined = progressCallback ? 
+      (update) => progressCallback(update as ProgressUpdate) : 
+      undefined;
+      
     return await measureFn(
       async (measure) => {
         const systemPrompt = this.config.systemPrompt || null;
@@ -434,7 +546,8 @@ export class Agent<I extends z.ZodObject<any>, O extends z.ZodObject<any>> {
         const response = await measure(() => callLLM(
           this.config.llm,
           messages,
-          { temperature: this.config.temperature || 0.7, maxTokens: this.config.maxTokens || 4000 },
+          { temperature: this.config.temperature || 0.7, maxTokens: this.config.maxTokens || 4000 },
-          measure
+          measure,
+          streamingCallback
         ), userPrompt);
         const parsed = await measure(
           async () => xmlToObj(response),
diff --git a/test/streaming.test.ts b/test/streaming.test.ts
new file mode 100644
index 0000000..1234567
--- /dev/null
+++ b/test/streaming.test.ts
@@ -0,0 +1,72 @@
+import { test, expect, it, describe } from 'bun:test';
+import { z } from 'zod';
+import { Agent, LLM, ProgressUpdate, StreamingUpdate } from '../uai';
+
+describe('UAI Streaming Tests', () => {
+  it('should emit streaming progress updates token-by-token', async () => {
+    const streamingAgent = new Agent({
+      llm: LLM['gpt-4o-mini'],
+      inputFormat: z.object({
+        question: z.string(),
+      }),
+      outputFormat: z.object({
+        analysis: z.string().describe("Step-by-step analysis of the question"),
+        answer: z.string().describe("The final answer to the question"),
+      }),
+      temperature: 0.7,
+    });
+
+    const input = {
+      question: 'What are the benefits of renewable energy?',
+    };
+
+    const streamingUpdates: StreamingUpdate[] = [];
+    const progressUpdates: ProgressUpdate[] = [];
+
+    try {
+      const result = await streamingAgent.run(input, (update) => {
+        if (update.stage === 'streaming') {
+          streamingUpdates.push(update as StreamingUpdate);
+          console.log(`Field: ${update.field}, Value: ${update.value.slice(-20)}...`);
+        } else {
+          progressUpdates.push(update);
+        }
+      });
+
+      console.log('\n--- Final Result ---');
+      console.log('Analysis:', result.analysis);
+      console.log('Answer:', result.answer);
+
+      // Verify we received streaming updates
+      expect(streamingUpdates.length).toBeGreaterThan(0);
+      
+      // Verify we have updates for both fields
+      const fieldsUpdated = new Set(streamingUpdates.map(u => u.field));
+      expect(fieldsUpdated.has('analysis')).toBe(true);
+      expect(fieldsUpdated.has('answer')).toBe(true);
+
+      // Verify the streaming updates show progressive content building
+      const analysisUpdates = streamingUpdates.filter(u => u.field === 'analysis');
+      const answerUpdates = streamingUpdates.filter(u => u.field === 'answer');
+      
+      expect(analysisUpdates.length).toBeGreaterThan(1);
+      expect(answerUpdates.length).toBeGreaterThan(1);
+
+      // Verify content is progressively building (each update should contain more text)
+      for (let i = 1; i < analysisUpdates.length; i++) {
+        expect(analysisUpdates[i].value.length).toBeGreaterThanOrEqual(analysisUpdates[i-1].value.length);
+      }
+
+      // Verify final result matches last streaming update values
+      const lastAnalysisUpdate = analysisUpdates[analysisUpdates.length - 1];
+      const lastAnswerUpdate = answerUpdates[answerUpdates.length - 1];
+      
+      expect(result.analysis.trim()).toBe(lastAnalysisUpdate.value.trim());
+      expect(result.answer.trim()).toBe(lastAnswerUpdate.value.trim());
+
+      console.log(`\n✅ Received ${streamingUpdates.length} streaming updates across ${fieldsUpdated.size} fields`);
+
+    } catch (error) {
+      console.warn('\n⚠️ Streaming test skipped (this is expected if API keys are not configured):', error.message);
+    }
+  }, { timeout: 60000 });
+});